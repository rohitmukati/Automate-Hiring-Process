from Data_Extraction.gemini_extractor import parse_resume
from Data_Extraction.links_extractor import classify_links
from Data_Extraction.github_extractor import extract_github_profiles

def run_pipeline(file_path):
    # âœ… Step 1: Resume data extract karo using Gemini
    resume_data = parse_resume(file_path)
    # print("\nðŸŸ¢ Resume Data:\n", resume_data)

    # âœ… Step 2: Links extract karo and classify karo
    # github_links, linkedin_links, certificate_links = classify_links(file_path)
    # print("\nðŸŸ¢ Extracted GitHub Links:\n", github_links)
    # print("\nðŸŸ¢ Extracted LinkedIn Links:\n", linkedin_links)
    # print("\nðŸŸ¢ Extracted Certificate Links:\n", certificate_links)

    # âœ… Step 3: GitHub se skills extract karo
    # if github_links:
    #     github_skills = extract_github_profiles(github_links)
        # print("\nðŸŸ¢ GitHub Skills:\n", github_skills)

    # âœ… Step 4: Final Output ko merge karo
    # resume_data['GitHub Skills'] = github_skills if github_links else []

    return resume_data

# if __name__ == "__main__":
#     file_path = 'Genai.pdf'
#     data = run_agent1(file_path)
#     print("\nâœ… Final Data:\n", data)
